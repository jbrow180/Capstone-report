---
title: "Capstone"

author: "Jasmine Brown"

date: "7/23/2019"

output:

  html_document: default

  pdf_document: default

---



Introduction

 Video games have been around for almost 70 years capturing the mind of people. There have been many popular remakes and changes made to the counsels and disk. The super Nintendo had the bulky games to insert in the top of the counsel how games are being downloaded from a play store. There is revenue from disk compatible with the Xbox, PlayStation, and Nintendo switch. However, selling these products may be challenging because of the change in video gaming and systems. It is more convenient to have a digital video game. Which is to download a game from an application store rather than going to the store on the night of a release and waiting in line. There could be a decrease in hardware sales because of the push to complete online. There is multiple fighting, racing, and other games all created to be released around the same time frame. There are also many new innovations in technology that gives a consumer access to games on other devices such as tablets, phones, and gaming computer. The data analysis of the video game sales with the genre, type, and conceal results could have an impact of the game next year. There are many questions to be answered with the data that is present.   Have there been a decrease in buying of video games? Are cellphones and the digital games taking over the video game industry? Since virtual video games have infiltrated the industry, the hypothesis that hardware sales have declined. 



The goal of this project is to determine the the number of games shipped based on the varibles presented, and also make assumptions about whats going on in the video game industry.The variables used are listed below:



Rank - the order of the games 

Name  - the names of the games

Platform - the names of the concels used to play the games

Total Shipped- the total of games shipped

Critic Score - The score that the critic gave the games

Year - The years that the gmaes were shipped

The necessary libraries needed for the project has been installed and are loaded here.
```{r}

library(tidyverse) #The swiss knife!

library(ggplot2)

library(caret)

library(rmarkdown)
```

All of the packages are loaded.  Therefore, we load the data into R and call the data frame dta.
```{r}
getwd()
Mydata <- read.csv("vsal.csv")

```





An extra variable was included in the data frame after the data was read.  Thus, the variable was removed.

```{r}

dataw<-select(dataw, -c(X))

```



Data Wrangligling



The data set was provided in a nearly cleaned manner.  However, multiple data wrangling techniques were performed on the data frame to transom it into a document that can be analyzed.  These techniques include removing unnecessary variables, changing the names of some columns, adding variables, and including functions for calculation. The view function gave us a table to view all 55,792 observation and twenty- three variables. Some of the variables were easier to understand that others. The ones such as Game, Rank, Name, Total shipped were plain. The variables such as Data score, Rating, and Sales did not have units specified. The dplry package came in handy when time to select the variables needed for the research’s Na. Omit function was also used to remove the variable with missing data points.The new dataset has 586 observations and 6 variables’ data is cleaned and save as a new set name Dataw. We can begin the data exploration by using the sapply and class on the dataset to view the types of data that are in each variable. There is a mix of numerical and factorial.




Exploratory Data Analysis

Now that the data have been ingested it is time to begin the exploration.  The first thing to do is to have a quick look at our data.  For that, the `str()` function and the more human-readable (and intuitive), `glimpse()` function. is used.



```{r}

glimpse(dataw)

```



As expected, there are 586 rows with 6 variables inside of the data set.  Additionally, the type of variables is apparent from this view.For example the Year could help tell home many games on average were shipped that year. Year is a numerical data. The platform is catergorical data that we can compare with the total shipped. 

```{r}

str(dataw)
```

Evaluating the output for each code, it is easy to determine that the codes provide almost identical information.  




```{r}
dta_hist <- ggplot(dataw, aes(Year))
dta_hist + geom_bar(aes(fill=Total_Shipped), width = 0.6, col="blue") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) + 
  labs(title="Total Shipped determined by years")
```





This filter seems to be a good one.  Now take a deeper dive into the data to determine the year by Total Shipped.  A comparison among year by total shipped will provide pertinent information.  The results will be displayed as a bar graph.In 2011 had the highest number of games shipped. After 2011 after that that year the number of games started to decline ass as whole. We made the Assumptions that after the years of the internet they may be started to be order online rather than being shipped. Looking at the variables total shipped versus platform.



```{r}
ggplot(datal, aes(x = Platform, y = Total_Shipped)) +
  geom_point() + geom_jitter()

```







On this graph we see that the Wii has a higher total shipped than any other platform. The data set id from the years of 1985 to 2019. That may have a large effect on the platforms with number f games shipped. We could make that assumptions that at that year the stores were popular with that game.



```{r}
ggplot(datal, aes(x = Rank, y = Total_Shipped)) +
  geom_point() 

```


On the graph we can that the higher the rank the more shipped.





```{r}

ggplot(datal, aes(x = Critic_Score, y = Total_Shipped, color="yellow")) + geom_point()


```





The Highest total shipped is for a critic score 7.9.




Statistical Analysis

Determine if there is a difference in means between the total shipped and the critic score for the variables "Score and Std_Score."  These two values are equivalent, but there may be value in examining both.





```{r}



dataw  %>%
  group_by(Platform) %>%
  summarise(n_games = n(),
            mean_shipped = mean(Total_Shipped),
            std_error = sd(Total_Shipped) / sqrt(n_games))



```









There is definitely a difference in means, as shown in the tables.  A t-test can provide further analysis. Compute a t.test to determine if the difference in means is statistically significant at conventional levels of confidence.   If p is larger than 0.05, accept the null that the two means are equal.  For a smaller p-value, more confidence is given when rejecting the null hypothesis.  Again, the t-test will be completed twice, once for "Score"  and another time for the "Std_Score" variable.



Recall:  Null hypotheses - the mean of the two samples are equal.    



```{r}

with(dta_new, t.test(Std_Score ~ Treatment))

with(dta_new, t.test(Score ~ Treatment))

```







Just as anticipated, p < 0.05, and the null hypothesis is rejected.  We continue the project by using the "MatchIt" package to completed the propensity score matching.  We get help from the "MathcIt" package to view the user guide to help determine the best fit for the code.





Machine Learning - Linear Regression

	The plot and models are now created for the understanding of the data. Since my dependent variable is a numerical value, linear regression will be used for the machine learning Technique. The best model to use was liner regression to explore the relationship between the dependent variables with the independent variables. Total shipped is the dependent variable We used the rest of the variables as independent variable to view how total shipped is changed. We can also write the relationship in a linear equation.
	
	
	
```{r}

validation_index <- createDataPartition(dataw$Total_Shipped, p=0.80, list=FALSE)
validation <- dataw[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataw <- dataw[validation_index,]


```

Before the linear model was created, we had created a validation data set to run the model on 80 percent and 20 percent to get the best outcome. The training data set is the 80% and the test data set is the 20 %.
	


```{r}

model29 = lm(Total_Shipped ~ Critic_Score + Year + Rank + Platform, data = dataw)
summary(model29)

summary(model29)


```

The model shows that the critic score, rank, platform, are significant variables to predict the total shipped, because the P vale is between 0 and 0.001. The Overall P value for the model is significant. A lower P value provides a better model. The multiple R square is 0.375. This variable show how close the independent variables are close to the line of best fit. The closer R squared is to 1 the better the model is therefore this model can be better. The Root mean squared error is an average of the variance. This number is based on the units of your independent variable when deciding its fate. It can range from 0 to any number. The smaller the number the better the model. My RMSE is 4.379 this this low compared to the average number of total shipped, there for this model is useful in predicting the total shipped but can be better.



A very important question when creating a model and exploring various feature combinations is accuracy. In order to measure accuracy, we use cross validation. The training data set get split into 5 parts randomly. The code below iterates 1 to 5 times.


```{r}

control <- trainControl(method="cv", number=5)
set.seed(7)
fit <- train(Total_Shipped~Critic_Score + Year + Rank + Platform, data=dataw, method="lm", metric="RMSE", trControl=control)
print(fit)

```

 
The cross validation shows accuracy because the RMSE and r squared are nearly the same as those in model9.to check additional accuracy we will run the model on validation set.


```{r}

model9 = lm(Total_Shipped ~ Critic_Score + Year + Rank + Platform, data = validation)
summary(model9)

```
 

 
This is the Model on the testing data which is twenty percent. This model does not give any significant variable. The P value is closer to 0. The RMSE is almost the same as the training data set. The r squared is 0.4. The model could be better.


Machine Learning Model

$$y=244.1x_1+.9708x_2-.1228x_3-0.0005044x_4+11.46x_5$$





Recommendations

1. Try to use more tactics to get the critic score up it is significant to total shipped.

2. Collect additional data on shipped in other countries that can be used as possible variables that influence purchases.

3. Higher me fulltime





Future Work

In the future I would like to be more particular in the research and look at Video games sales specifically in other countries.











